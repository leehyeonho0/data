{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링 실행 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv\n",
    "import requests # URL 주소에 있는 내용을 요청할 때 사용하는 모듈\n",
    "import urllib.request as ur  # 웹에서 얻은 데이터를 다루는 파이썬 패키지\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(ur.urlopen('https://news.daum.net/').read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.v.daum.net/v/20210216135846443\n",
      "https://news.v.daum.net/v/20210216134601000\n",
      "https://news.v.daum.net/v/20210216134500965\n",
      "https://news.v.daum.net/v/20210216120058888\n"
     ]
    }
   ],
   "source": [
    "# URL 주소 저장하기\n",
    "os.chdir(r'D:\\빅데이터분석전문가\\강의자료\\텍스트분석\\crawling_save')\n",
    "f = open('links.txt', 'w')  # 'links.txt' 라는 제목의 쓰기 전용 파일을 연다.\n",
    "for i in soup.find_all('div', {\"class\" : \"item_issue\"}):\n",
    "    f.write(i.find_all('a')[0].get('href')+'\\n')\n",
    "    print(i.find_all('a')[0].get('href'))\n",
    "\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이명박 정부 시절 국가정보원의 불법 사찰 의혹과 관련해 여당이 신상규명 결의안을 발의하자, 야당은 4월 보궐선거용 정치공세라고 맞섰다.\n",
      "국회 정보위원회 더불어민주당 간사인 김병기 의원은 16일 '국가정보기관의 사찰성 정보 공개 촉구 및 진상 규명과 재발 방지를 위한 특별 결의안'을 대표 발의했다.\n",
      "Translated by kakao i\n",
      "[서울신문]\n",
      "\n",
      "이명박 정부 시절 국가정보원의 불법 사찰 의혹과 관련해 여당이 신상규명 결의안을 발의하자, 야당은 4월 보궐선거용 정치공세라고 맞섰다.\n",
      "국회 정보위원회 더불어민주당 간사인 김병기 의원은 16일 ‘국가정보기관의 사찰성 정보 공개 촉구 및 진상 규명과 재발 방지를 위한 특별 결의안’을 대표 발의했다.\n",
      "김 의원은 “국정원의 사찰성 정보 공개 및 사과를 촉구하는 결의안을 51명의 의님들과 함께 발의했다”면서 “국회는 정보기관으로부터 일어나는 국민의 기본권과 민주주의에 대한 침해로부터 방파제의 역할을 할 것”이라고 밝혔다.\n",
      "결의안 공동 발의 명단엔 김 의원과 이낙연 대표, 김태년 원내대표도 이름을 올렸다.\n",
      "해당 결의안은 국정원은 사찰 피해자에게 선제적으로 사찰성 정보를 공개 및 폐기하고, 국정원을 비롯한 각 정보기관 등의 사과 및 재발 방지 노력 등의 내용을 담고 있다.\n",
      "김 의원은 “정보주체로서 모든 국민에게 자신에 대한 정보를 자율적으로 통제할 수 있는 적극적인 권리가 있고 이는 헌법적 권리”라며 “국가정보원에 대해 사찰성 정보를 당사자에게 선제적으로 공개하고, 진상 규명과 재발 방지를 위해 노력할 것을 촉구한다”고 말했다.\n",
      "앞서 여당은 이명박 정부 당시 국정원이 18대 국회의원, 언론계, 법조계 등 불법사찰을 폭넓게 진행했다며 의혹을 제기한 바 있다. 이에 민주당 일부 의원은 개인적으로 정보공개 청구에 나섰다.\n",
      "\n",
      "반면 야당은 이에 대해 선거용 흑색선전이라고 비판했다.\n",
      "이종배 국민의힘 정책위의장은 이날 원내대책 회의에서 “재보궐 선거를 50여일 앞둔 시점에서 민주당이 상습적인 전 정부 탓, 그것을 넘어서는 저급한 마타도어를 하고 있다”며 유감스럽다는 뜻을 밝혔다.\n",
      "이어 “전날 이낙연 대표는 MB정부에서 국정원 불법사찰이 있었다며 중대 범죄라고 맹비난했는데 정작 이 대표는 김은경 전 환경부 장관이 불법사찰과 블랙리스트로 중형을 선고받은 데 대해서는 안타깝다는 입장 외엔 침묵을 지켰다”며 “민주당 정권의 불법사찰에 대해선 일언반구 언급도 못 하는 분이 난데없이 12년 전 정권 일을 끄집어내 불법사찰 정치공세에 나섰다”고 지적했다.\n",
      "이명박 정부 시절 청와대 정무수석을 지낸 정진석 국민의힘 의원도 전날 페이스북에 “문재인 정부 들어 ‘적폐청산’을 명분으로 국정원 메인컴퓨터는 물론 직원들의 컴퓨터까지 탈탈 털렸는데 그때도 나오지 않던 국회의원 동향사찰 문건이 갑자기 어디서 쑥 튀어나왔는가 보다”며 “마침 국정원장이 정치적 술수의 대가로 알려진 박지원 전 의원”이라고 비판했다.\n",
      "윤창수 기자 geo@seoul.co.kr\n",
      "해당 언론사로 연결됩니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기사 본문을 파일로 저장하기\n",
    "article1 = 'https://news.v.daum.net/v/20210216134601000'\n",
    "\n",
    "soup = bs(ur.urlopen(article1).read(), 'html.parser')\n",
    "f = open('article_1.txt','w')\n",
    "for i in soup.find_all('p'):\n",
    "    f.write(i.text+'\\n')\n",
    "    print(i.text)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 제목, 본문, 하이퍼링크 파일로 저장하기\n",
    "url = 'https://news.daum.net/'\n",
    "soup = bs(ur.urlopen(url).read(), 'html.parser')\n",
    "\n",
    "f = open('article_total.txt','w')\n",
    "\n",
    "for i in soup.find_all('div', {\"class\" : \"item_issue\"}):\n",
    "    try:\n",
    "        f.write(i.text + '\\n')  # 기사 제목\n",
    "        f.write(i.find_all('a')[0].get('href') + '\\n')  # 기사 URL링크\n",
    "        soup2 = bs(ur.urlopen(i.find_all('a')[0].get('href')).read(), 'html.parser')\n",
    "        for j in soup2.find_all('p'):\n",
    "            f.write(j.text)   # 기사 본문\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지금까지 작성한 코드를 파이썬 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyinstaller\n",
      "  Downloading pyinstaller-4.2.tar.gz (3.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\028\\anaconda3\\lib\\site-packages (from pyinstaller) (50.3.1.post20201107)\n",
      "Collecting altgraph\n",
      "  Downloading altgraph-0.17-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pyinstaller-hooks-contrib>=2020.6\n",
      "  Downloading pyinstaller_hooks_contrib-2020.11-py2.py3-none-any.whl (172 kB)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0; sys_platform == \"win32\" in c:\\users\\028\\anaconda3\\lib\\site-packages (from pyinstaller) (0.2.0)\n",
      "Collecting pefile>=2017.8.1; sys_platform == \"win32\"\n",
      "  Downloading pefile-2019.4.18.tar.gz (62 kB)\n",
      "Requirement already satisfied: future in c:\\users\\028\\anaconda3\\lib\\site-packages (from pefile>=2017.8.1; sys_platform == \"win32\"->pyinstaller) (0.18.2)\n",
      "Building wheels for collected packages: pyinstaller, pefile\n",
      "  Building wheel for pyinstaller (PEP 517): started\n",
      "  Building wheel for pyinstaller (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyinstaller: filename=pyinstaller-4.2-py3-none-any.whl size=2413076 sha256=3bf5caf646763bdc9677d86d5e58e7d68736740eb80e93660d76976911304930\n",
      "  Stored in directory: c:\\users\\028\\appdata\\local\\pip\\cache\\wheels\\65\\6f\\54\\0f682e8590de992d07a17ce07282267734cb150e537dfc4390\n",
      "  Building wheel for pefile (setup.py): started\n",
      "  Building wheel for pefile (setup.py): finished with status 'done'\n",
      "  Created wheel for pefile: filename=pefile-2019.4.18-py3-none-any.whl size=60828 sha256=0e0fa0e63eb1271178ac176391dc2f2dd7892d2c3bb3bc8fbf993e29c8474d07\n",
      "  Stored in directory: c:\\users\\028\\appdata\\local\\pip\\cache\\wheels\\42\\52\\d5\\9550bbfb9eeceaf0f19db1cf651cc8ba41d3bcf8b4d20e4279\n",
      "Successfully built pyinstaller pefile\n",
      "Installing collected packages: altgraph, pyinstaller-hooks-contrib, pefile, pyinstaller\n",
      "Successfully installed altgraph-0.17 pefile-2019.4.18 pyinstaller-4.2 pyinstaller-hooks-contrib-2020.11\n"
     ]
    }
   ],
   "source": [
    "# pyinstaller 설치\n",
    "#!pip install pyinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비주얼스튜디오로 코드 저장 (WebCrawling.py)\n",
    "import os, re, csv\n",
    "import requests # URL 주소에 있는 내용을 요청할 때 사용하는 모듈\n",
    "import urllib.request as ur  # 웹에서 얻은 데이터를 다루는 파이썬 패키지\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 기사 제목, 본문, 하이퍼링크 파일로 저장하기\n",
    "url = 'https://news.daum.net/'\n",
    "soup = bs(ur.urlopen(url).read(), 'html.parser')\n",
    "\n",
    "f = open('article_total.txt','w')\n",
    "\n",
    "for i in soup.find_all('div', {\"class\" : \"item_issue\"}):\n",
    "    try:\n",
    "        f.write(i.text + '\\n')  # 기사 제목\n",
    "        f.write(i.find_all('a')[0].get('href') + '\\n')  # 기사 URL링크\n",
    "        soup2 = bs(ur.urlopen(i.find_all('a')[0].get('href')).read(), 'html.parser')\n",
    "        for j in soup2.find_all('p'):\n",
    "            f.write(j.text)   # 기사 본문\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 파일 만들기 : IDLE, 파이참 또는 비주얼 스튜디오 등을 열어 실행\n",
    "pyinstaller --onefile WebCrawling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist폴더에 실행파일 생섬됨 (WebCrawling.exe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

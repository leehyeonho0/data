{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 11-1. Word Embeddings with GloVe and Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe\n",
    "    - window 안에 속하는 단어들만을 반영하는 word2vec의 단점을 해결하기 위한 아이디어\n",
    "    - 전체 dictionary에서 두 단어의 동시등장(co-occurrence)하는 확률을 계산하고 동시에 등장하는 확률이 높을 수록 두 단어 벡터가 가까워지도록 학습\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python-binary\n",
      "  Downloading glove_python_binary-0.2.0-cp37-cp37m-win_amd64.whl (225 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\tj\\anaconda3\\lib\\site-packages (from glove_python-binary) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tj\\anaconda3\\lib\\site-packages (from glove_python-binary) (1.18.1)\n",
      "Installing collected packages: glove-python-binary\n",
      "Successfully installed glove-python-binary-0.2.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install glove_python-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
    "              ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터는 각 문장을 단어들의 list로 표현하여 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
       " ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() \n",
    "corpus.fit(input_text, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Co-occurrence를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'da': 1,\n",
       " 'vinci': 2,\n",
       " 'code': 3,\n",
       " 'book': 4,\n",
       " 'is': 5,\n",
       " 'just': 6,\n",
       " 'awesome': 7,\n",
       " '.': 8,\n",
       " 'i': 9,\n",
       " 'liked': 10,\n",
       " 'a': 11,\n",
       " 'lot': 12}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t0.5\n",
      "  (0, 2)\t0.5\n",
      "  (0, 3)\t0.6666666865348816\n",
      "  (0, 4)\t0.25\n",
      "  (0, 5)\t0.20000000298023224\n",
      "  (0, 6)\t0.1666666716337204\n",
      "  (0, 7)\t0.1428571492433548\n",
      "  (0, 8)\t0.2916666865348816\n",
      "  (0, 9)\t0.5\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t0.25\n",
      "  (0, 12)\t0.20000000298023224\n",
      "  (1, 2)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 4)\t0.3333333432674408\n",
      "  (1, 5)\t0.25\n",
      "  (1, 6)\t0.20000000298023224\n",
      "  (1, 7)\t0.1666666716337204\n",
      "  (1, 8)\t0.34285715222358704\n",
      "  (1, 9)\t0.3333333432674408\n",
      "  (1, 10)\t0.5\n",
      "  (1, 11)\t0.3333333432674408\n",
      "  (1, 12)\t0.25\n",
      "  :\t:\n",
      "  (3, 8)\t0.5333333611488342\n",
      "  (3, 9)\t0.20000000298023224\n",
      "  (3, 10)\t0.25\n",
      "  (3, 11)\t1.0\n",
      "  (3, 12)\t0.5\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t0.5\n",
      "  (4, 7)\t0.3333333432674408\n",
      "  (4, 8)\t0.25\n",
      "  (5, 6)\t1.0\n",
      "  (5, 7)\t0.5\n",
      "  (5, 8)\t0.3333333432674408\n",
      "  (6, 7)\t1.0\n",
      "  (6, 8)\t0.5\n",
      "  (7, 8)\t1.0\n",
      "  (8, 9)\t0.125\n",
      "  (8, 10)\t0.1428571492433548\n",
      "  (8, 11)\t0.5\n",
      "  (8, 12)\t1.0\n",
      "  (9, 10)\t1.0\n",
      "  (9, 11)\t0.1666666716337204\n",
      "  (9, 12)\t0.1428571492433548\n",
      "  (10, 11)\t0.20000000298023224\n",
      "  (10, 12)\t0.1666666716337204\n",
      "  (11, 12)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(corpus.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove(no_components=5)\n",
    "glove.fit(corpus.matrix, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 생성한 co-occurrence matrix를 입력값으로 받아 glove 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04568983, -0.10630906, -0.06833508, -0.00521731,  0.02551073],\n",
       "       [-0.02210793, -0.07019449, -0.00827241, -0.01887154, -0.06760364],\n",
       "       [-0.07122593, -0.03427791, -0.08239786, -0.02330353, -0.02362877],\n",
       "       [-0.00138935,  0.09314505,  0.05422496, -0.02673546,  0.0828445 ],\n",
       "       [-0.10027092,  0.0950792 , -0.09305676,  0.04907259,  0.02864791],\n",
       "       [ 0.07688888,  0.09343062, -0.07319521, -0.05993485, -0.07205006],\n",
       "       [ 0.04726897,  0.00566121,  0.08023728,  0.07804979, -0.01328752],\n",
       "       [ 0.05259584, -0.04637605,  0.07081554,  0.05173817, -0.03134926],\n",
       "       [ 0.00664143, -0.08133186,  0.10216001, -0.02002165,  0.06465788],\n",
       "       [ 0.09795657,  0.02245815,  0.0151073 , -0.08012637, -0.04895071],\n",
       "       [ 0.0024732 , -0.04678153, -0.02913527,  0.09285719, -0.06221956],\n",
       "       [-0.02355928,  0.02773438, -0.00809839,  0.04283896, -0.03434178],\n",
       "       [-0.02863654,  0.06917519, -0.00346973, -0.07914805, -0.05833579]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.word_vectors # embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서를 line별로 읽어들이면서 단어의 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_FEATURES = 2000   \n",
    "MAX_SENTENCE_LENGTH = 40  \n",
    "\n",
    "import collections\n",
    "import os \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)  # the maximum number of words in a sentence\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1  # frequency for each word\n",
    "    num_recs += 1 # total number of records\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 등장 빈도를 기준으로 `MAX_FEATURES` 만큼의 단어를 vocabulary로 결정\n",
    "- vocabulary에 속하지 않는 단어는 \"UNK\"로 표시하면서 문장을 단어 단위로 tokenize 하고 list로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:03:10.031128Z",
     "start_time": "2018-07-10T17:03:09.018550Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [v for v, _ in word_freqs.most_common(MAX_FEATURES)]\n",
    "\n",
    "sentences = np.empty((num_recs, ), dtype=list)\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            sentence.append(\"UNK\")\n",
    "    sentences[i] = sentence\n",
    "    i += 1\n",
    "    \n",
    "ftrain.close()\n",
    "\n",
    "sentences=list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7086"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: GloVe 알고리즘을 통해 word embedding을 하고 embedding matrix를 `embedding_matrix_glove`의 이름으로 저장하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "corpus = Corpus() \n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=EMBEDDING_SIZE)\n",
    "glove.fit(corpus.matrix, epochs=30)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = glove.word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: Word2vec 알고리즘을 사용하는 Lecture 11의 예제와 동일하게 이후 과정 진행\n",
    "\n",
    "- Embedding matrix에 \"UNK\"을 나타내는 0 행을 추가 \n",
    "- Look-up dictionary 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = np.append(np.zeros((1,EMBEDDING_SIZE)), embedding_matrix_glove, axis=0)\n",
    "\n",
    "index2word = {i+1: w for i, w in enumerate(glove.dictionary)} \n",
    "index2word[0] = 'PAD'\n",
    "word2index = {w: i for i, w in index2word.items() }\n",
    "\n",
    "vocab_size = len(index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras embedding layer에 입력하기 위해 단어 인덱스를 사용하여 문장을 list로 변환하여 저장하고 각 문장의 sentiment label 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 40) (1418, 40) (5668,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: \n",
    "\n",
    "GloVe에 의해 학습된 embedding matrix를 사용하여 word2vec를 사용해 학습했던 모형과 동일한 구조의 모형을 학습하고 test set에 대한 accuracy를 계산하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 128)           256256    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 276,897\n",
      "Trainable params: 20,641\n",
      "Non-trainable params: 256,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length = MAX_SENTENCE_LENGTH, mask_zero = True,\n",
    "                    weights = [embedding_matrix_glove], trainable = False))\n",
    "model.add(LSTM(32, recurrent_dropout = 0.2, return_sequences = False))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TJ\\anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.\n",
      "  warnings.warn('The TensorBoard callback does not support '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/100\n",
      "5668/5668 [==============================] - 2s 378us/step - loss: 0.6522 - acc: 0.6316 - val_loss: 0.6084 - val_acc: 0.6798\n",
      "Epoch 2/100\n",
      "5668/5668 [==============================] - 1s 231us/step - loss: 0.5700 - acc: 0.7378 - val_loss: 0.5107 - val_acc: 0.7581\n",
      "Epoch 3/100\n",
      "5668/5668 [==============================] - 1s 210us/step - loss: 0.4663 - acc: 0.8126 - val_loss: 0.4022 - val_acc: 0.8547\n",
      "Epoch 4/100\n",
      "5668/5668 [==============================] - 1s 200us/step - loss: 0.3917 - acc: 0.8439 - val_loss: 0.3493 - val_acc: 0.8526\n",
      "Epoch 5/100\n",
      "5668/5668 [==============================] - 1s 191us/step - loss: 0.3457 - acc: 0.8634 - val_loss: 0.3053 - val_acc: 0.8731\n",
      "Epoch 6/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.3043 - acc: 0.8829 - val_loss: 0.2767 - val_acc: 0.8858\n",
      "Epoch 7/100\n",
      "5668/5668 [==============================] - 1s 205us/step - loss: 0.2815 - acc: 0.8848 - val_loss: 0.2447 - val_acc: 0.9013\n",
      "Epoch 8/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.2532 - acc: 0.9031 - val_loss: 0.2214 - val_acc: 0.9154\n",
      "Epoch 9/100\n",
      "5668/5668 [==============================] - 1s 204us/step - loss: 0.2352 - acc: 0.9070 - val_loss: 0.2086 - val_acc: 0.9013\n",
      "Epoch 10/100\n",
      "5668/5668 [==============================] - 1s 192us/step - loss: 0.2232 - acc: 0.9171 - val_loss: 0.2013 - val_acc: 0.9189\n",
      "Epoch 11/100\n",
      "5668/5668 [==============================] - 1s 202us/step - loss: 0.2127 - acc: 0.9231 - val_loss: 0.1860 - val_acc: 0.9351\n",
      "Epoch 12/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.2000 - acc: 0.9271 - val_loss: 0.1781 - val_acc: 0.9351\n",
      "Epoch 13/100\n",
      "5668/5668 [==============================] - 1s 200us/step - loss: 0.1866 - acc: 0.9342 - val_loss: 0.1699 - val_acc: 0.9365\n",
      "Epoch 14/100\n",
      "5668/5668 [==============================] - 1s 191us/step - loss: 0.1770 - acc: 0.9375 - val_loss: 0.1635 - val_acc: 0.9351\n",
      "Epoch 15/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.1674 - acc: 0.9393 - val_loss: 0.1535 - val_acc: 0.9394\n",
      "Epoch 16/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.1625 - acc: 0.9405 - val_loss: 0.1520 - val_acc: 0.9358\n",
      "Epoch 17/100\n",
      "5668/5668 [==============================] - 1s 183us/step - loss: 0.1576 - acc: 0.9407 - val_loss: 0.1460 - val_acc: 0.9429\n",
      "Epoch 18/100\n",
      "5668/5668 [==============================] - 1s 184us/step - loss: 0.1504 - acc: 0.9435 - val_loss: 0.1401 - val_acc: 0.9408\n",
      "Epoch 19/100\n",
      "5668/5668 [==============================] - 1s 184us/step - loss: 0.1433 - acc: 0.9485 - val_loss: 0.1375 - val_acc: 0.9485\n",
      "Epoch 20/100\n",
      "5668/5668 [==============================] - 1s 176us/step - loss: 0.1437 - acc: 0.9460 - val_loss: 0.1344 - val_acc: 0.9436\n",
      "Epoch 21/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.1378 - acc: 0.9462 - val_loss: 0.1271 - val_acc: 0.9499\n",
      "Epoch 22/100\n",
      "5668/5668 [==============================] - 1s 183us/step - loss: 0.1328 - acc: 0.9517 - val_loss: 0.1235 - val_acc: 0.9520\n",
      "Epoch 23/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.1308 - acc: 0.9517 - val_loss: 0.1241 - val_acc: 0.9549\n",
      "Epoch 24/100\n",
      "5668/5668 [==============================] - 1s 208us/step - loss: 0.1286 - acc: 0.9501 - val_loss: 0.1178 - val_acc: 0.9520\n",
      "Epoch 25/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.1232 - acc: 0.9552 - val_loss: 0.1207 - val_acc: 0.9520\n",
      "Epoch 26/100\n",
      "5668/5668 [==============================] - 1s 183us/step - loss: 0.1223 - acc: 0.9573 - val_loss: 0.1181 - val_acc: 0.9528\n",
      "Epoch 27/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.1178 - acc: 0.9562 - val_loss: 0.1154 - val_acc: 0.9513\n",
      "Epoch 28/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.1141 - acc: 0.9568 - val_loss: 0.1133 - val_acc: 0.9542\n",
      "Epoch 29/100\n",
      "5668/5668 [==============================] - 1s 183us/step - loss: 0.1082 - acc: 0.9605 - val_loss: 0.1127 - val_acc: 0.9549\n",
      "Epoch 30/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.1046 - acc: 0.9612 - val_loss: 0.1037 - val_acc: 0.9577\n",
      "Epoch 31/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.0997 - acc: 0.9642 - val_loss: 0.1042 - val_acc: 0.9577\n",
      "Epoch 32/100\n",
      "5668/5668 [==============================] - 1s 202us/step - loss: 0.1029 - acc: 0.9624 - val_loss: 0.1029 - val_acc: 0.9577\n",
      "Epoch 33/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.0933 - acc: 0.9672 - val_loss: 0.1016 - val_acc: 0.9605\n",
      "Epoch 34/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.0990 - acc: 0.9629 - val_loss: 0.0994 - val_acc: 0.9598\n",
      "Epoch 35/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.0987 - acc: 0.9621 - val_loss: 0.0990 - val_acc: 0.9605\n",
      "Epoch 36/100\n",
      "5668/5668 [==============================] - 1s 203us/step - loss: 0.0890 - acc: 0.9695 - val_loss: 0.0950 - val_acc: 0.9584\n",
      "Epoch 37/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.0922 - acc: 0.9663 - val_loss: 0.0952 - val_acc: 0.9598\n",
      "Epoch 38/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.0888 - acc: 0.9682 - val_loss: 0.0953 - val_acc: 0.9605\n",
      "Epoch 39/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.0880 - acc: 0.9693 - val_loss: 0.0977 - val_acc: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x246c8c37048>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='models/sentiment_analysis_glove.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='logs/sentiment_analysis_glove/', histogram_freq=1, embeddings_freq=1,\n",
    "                # histogram_freq=1 : 1 에포크마다 활성화 출력의 히스토그램을 기록\n",
    "                # embeddings_freq=1 : 1에포크 마다 임베딩 데이터 기록\n",
    "                # profile_batch : https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "               profile_batch=100000000),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 70us/step\n",
      "Test loss: 0.098, accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test loss: %.3f, accuracy: %.3f\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\tso brokeback mountain was really depressing .\n",
      "0\t0\tmy dad 's being stupid about brokeback mountain ...\n",
      "0\t0\toh , and brokeback mountain was a terrible movie .\n",
      "0\t0\tas i sit here , watching the mtv movie awards , i am reminded of how much i despised the movie brokeback mountain .\n",
      "0\t0\tas i sit here , watching the mtv movie awards , i am reminded of how much i despised the movie brokeback mountain .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
